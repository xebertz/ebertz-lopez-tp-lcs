= TP Inicial - Laboratorio de Construcción de Software
Ebertz Ximena <xebertz@campus.ungs.edu.ar>; Gross Pablo <pablorubengross@gmail.com>; López Gonzalo <gonzagonzalopez20@gmail.com>
v1, {docdate}
:toc:
:title-page:
:toc-title: Secciones
:numbered:
:source-highlighter: highlight.js
:tabsize: 4
:nofooter:
:pdf-page-margin: [3cm, 3cm, 3cm, 3cm]

== Introducción

En el presente documento, se abordará el propósito central de nuestro sistema, explicando las razones detrás de la elección del modelo de Deep Learning seleccionado, detallando las dificultades que enfrentamos durante el proceso y describiendo el procedimiento de entrenamiento del modelo.

== Desarrollo

=== Detección de lunares benignos y malignos

El propósito central de nuestro sistema radica en la detección precisa de lunares benignos y malignos a través del análisis de imágenes. Estas imágenes son adquiridas de la página web https://www.kaggle.com/[Kaggle], concretamente del conjunto de datos disponible en https://www.kaggle.com/datasets/fanconic/skin-cancer-malignant-vs-benign[Skin Cancer: Malignant vs. Benign]. Este conjunto específico consta de un total de 2637 imágenes utilizadas para el entrenamiento, distribuidas en 1440 imágenes de lunares benignos y 1197 imágenes de lunares malignos. Además, se dispone de 660 imágenes para llevar a cabo pruebas, compuestas por 360 imágenes de lunares benignos y 300 imágenes de lunares malignos.

=== Pre-Entrenamiento

Antes de llevar a cabo el entrenamiento del modelo, fue necesario ejecutar una serie de pasos mediante código para asegurar su viabilidad y efectividad.

Inicialmente, procedimos a descargar todas las imágenes disponibles desde la página web previamente mencionada.

Posteriormente, organizamos estas imágenes en listas separadas, categorizándolas en función de si serían destinadas para el entrenamiento o la fase de pruebas. Además, aplicamos una estandarización en las dimensiones, ajustando cada imagen a un formato de JPG de 100x100 pixeles 
// TENEMOS QUE DEFINIRLO
NxN.
Este enfoque se eligió para evitar consumir excesiva memoria RAM en el entorno de Google Colab.

Luego, cada imagen fue etiquetada en consecuencia. Aquellas que representaban lunares benignos se etiquetaron con un valor de 0, mientras que las imágenes de carácter maligno se etiquetaron con un valor de 1.

Con el propósito de evitar sesgos en el modelo, implementamos una etapa de mezcla de las imágenes. Esta mezcla se llevó a cabo de manera que las etiquetas continuaran alineadas correctamente. De esta manera, se evitó que el modelo recibiera secuencias de imágenes en las que las muestras benignas o malignas estuvieran agrupadas en bloques.

Además, llevamos a cabo una etapa de normalización en las imágenes. Esta normalización ajustó los valores de los píxeles en un rango entre 0 y 1, lo que resulta fundamental para un procesamiento y entrenamiento más eficiente del modelo.

Una vez completados estos pasos, estuvimos en condiciones de comenzar con el proceso de entrenamiento y llevar a cabo pruebas para evaluar el rendimiento del modelo resultante.

=== Entrenamiento de modelos

Realizamos el entrenamiento de redes neuronales densas y convolucionales, y a continuación, compartiremos las configuraciones de parámetros que empleamos para estas distintas redes, así como aquella que determinamos como el modelo óptimo.

==== Configuración de parámetros

===== Red neuronal densa

Esta red neuronal densa esta configurada de tal manera que se tiene una capa de entrada de 10.000 neuronas, correspondiendo cada una de estas a un pixel de la imagen de 100x100 pixeles.

Despues se tiene dos capas ocultas que contienen 150 neuronas cada una, las cuales se encargan de analizar los datos de las neuronas de entrada.

Por ultimo se encuentra una sola neurona de salida la cual determina con un 1 o un 0 (redondeando los resultados intermedios) si el lunar de la imagen analizada es maligno o benigno.

por ende este modelo solo llega a alcanzar como maximo un 76/77% de presicion. La cual implicaria un alto nivel de presicion, pero por como actua la red neuronal densa es que pierde bastante el contexto de las imagenes dadas. Por lo tanto al procesar informacion la cual se encuentra fuera de los rasgos de las imagenes de entrenamiento pierde eficacia y precision.

===== Red neuronal convolucional

Esta red neuronal convolucional esta configura de tal manera la cual se tiene tres capas convolucionales en la entrada. Las cuales se encargan de observar la imagen en clusters de 3x3 pixeles 3 veces para poder comprimirlas manteniendo las caracteristicas mas importantes de la misma. Para asi poder procesarla mas rapido en las capas subsiguientes.

Despues se encuentra la capa de dropdown la cual modifica los resultados los nodos a los cuales se dirigen los resultados para evitar sobrecompensacion en los resultados. 

Siguiente a esto se genera la capa de entrada que toma la imagen comprimida y genera una neurona de entrada por cada pixel de la misma imagen para asi poder porcesarla.

A continuacion se encuentra una capa oculta de 25 neuronas para poder procesar los datos de las imagenes.

Por ultimo se encuentra la capa de salida la cual es una sola neurona la cual determina con un 1 o un 0 si el lunar es maligno o benigno.

Por las caracteristicas de las capas convolucionales se puede intuir que es recomendable usarla para el analizis de imagenes ya que estas le permiten añadir contexto espacial a la prediccion del modelo neuronal. Por esto mismo, el modelo que estamos usando llega a una presicion del 80/81% 

==== Modelo óptimo

Por lo mencionado previamente en la explicacin de los modelos usados, se puede finalmente llegar a a conclucion que para la tarea a completar, la cual consiste en analizar fotos, es mas optima la red neuronal convolucional. Esto se deve a que presenta un nivel mayor de presicion y permite que con el entrenamiento presentado para el modelo pueda intuir y determinar un resultado de una imagen con la cual no entreno y no sea completamente similiar a un dato de entrenamiento.

Entrando en mas detalle, la red neuronal densa en su aprendisaje puede llegar a un 78% de presicion pero este resultado no se presenta en el testeo con datos aleatorios de los cuales no aprendio, lo que genera una variacion grande en los resultados de sus prediciones.

Por otra parte la red neuronal convolucional quiza tarde mas en su entrenamiento pero llega a un porcentaje de presicion del 81% el cual tambien se traslada a ejemplos del mundo real con datos aleatorios de los cuales no se encontraban en los datos de entrenamiento. a su ves por el tipo de aprendisaje de contexto en las imagenes permite una mayor consistencia en sus resultados el cual tambien es 81%.

== Conclusión

...