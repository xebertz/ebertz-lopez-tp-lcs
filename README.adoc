= TP Inicial - Laboratorio de Construcción de Software
Ebertz Ximena <xebertz@campus.ungs.edu.ar>; Gross Pablo <pablorubengross@gmail.com>; López Gonzalo <gonzagonzalopez20@gmail.com>
v1, {docdate}
:toc:
:title-page:
:toc-title: Secciones
:numbered:
:source-highlighter: coderay
:tabsize: 4
:nofooter:
:stem: asciimath
:pdf-page-margin: [3cm, 3cm, 3cm, 3cm]


== Introducción

Investigación de conceptos: IA, Machine Learning, Deep Learning.

Investigación de proyectos de Data Science o Big Data.

== Objetivo

Acá va el objetivo.

== Repositorio

Para ir al repositorio del proyecto puede darle click https://github.com/ximeeb/ebertz-gross-lopez-tp-lcs.git[aquí].

== Desarrollo

=== Inteligencia Artificial (IA)

Antes de profundizar en el concepto de _Inteligencia Artificial_, debemos entender qué es un _agente_. La definición de este término es sencilla: un _agente_ es "alguien" hace algo. En el ámbito informático extendemos esta definción a la de _agentes computacionales_, que son programas que pueden operar autónomamente, percibir su entorno, persisitir durante un período de tiempo, adaptarse al cambio, y crear y perseguir metas. Por esto, podemos observar que no todos los sistemas informáticos son agentes.

En este contexto, conocemos como _Inteligencia Artificial_ o _IA_ a los *agentes computacionales racionales* existentes. Es decir, IA es cualquier software capaz de percibir su entorno, persistir durante un periodo de tiempo, adaptarse al cambio, y realizar cualquier cosa de la que es capaz un agente computacional, pero con la característica de que responde y actúa de manera racional, buscando la mejor opción para cada situación. Muchos de estos sistemas tienen como objetivo imitar o incluso superar la inteligencia humana, replicando sus funciones cognitivas tales como razonar, aprender y resolver problemas.

Dentro de la inteligencia artificial tenemos dos tipos: la _inteligencia artificial débil_, y la _inteligencia artificial fuerte_. La inteligencia artificial débil es la que no pretende parecerse a la inteligencia humana en su totalidad. Su finalidad es resolver un problema, y no más de uno. La inteligencia artificial fuerte es la que pretende parecerse o superar a la inteligencia humana, de forma que podría resolver cualquier problema que un ser humano puede resolver. Este tipo de inteligencia artificial en la actualidad se mantiene como hipotética.

Se reconocen cuatro etapas o niveles de desarrollo de inteligencia artificial:

*   _máquinas reactivas:_ es un tipo de inteligencia artificial que no puede aprender ya que no utiliza memoria, pero sabe reaccionar ante ciertos estímulos, ya que lo tiene preprogramado.
*   _de memoria limitada:_ es un tipo de inteligencia artificial que puede aprender, ya que utiliza memoria. Para generar su conocimiento se la entrena brindándole datos nuevos.
*   _de teoría de la mente:_ es un tipo de inteligencia artificial que puede emular la mente humana, lo que le da la capacidad de tener sentimientos y emociones.
*   _de autoconocimiento:_ es una inteligencia artificial de teoría de la mente con autoconocimiento.

Los sistemas de inteligencia artificial actuales tienen una particularidad: son para un uso específico. Es decir, son inteligencias artificiales débiles, y, en particular, de memoria limitada. No existen sistemas en los que se pueda hacer por ejemplo, reconocimiento de voz, jugar al ajedrez y realizar predicciones de un tema específico. Cada inteligencia artificial tiene un objetivo bien definido. Si el objetivo es jugar al ajedrez, la inteligencia artificial será entrenada para esto solamente, por lo que recibirá una fuente grande de datos de este tema sobre la que forjará su aprendizaje.

Las IA están entrenadas para resolver problemas utilizando algoritmos, aprender de los datos que se le brindaron, aprender de sus errores, y utilizar ese conocimiento en el futuro.

El entrenamiento de una inteligencia artificial consiste en brindarle datos y analizar su comportamiento, con la finalidad de ajustar sus parámetros y alcanzar cierto umbral de satisfacción. Existen dos formas de entrenamiento: _Machine Learning_ y _Deep Learning_.

=== Machine Learning

El _Machine Learning_ o _Aprendizaje Automático_ es una forma de entrenar a las inteligencias artificiales (IA). En lugar de programar explícitamente reglas o instrucciones para realizar una tarea, se utilizan algoritmos que pueden ajustar automáticamente sus parámetros en función de los datos proporcionados. Esto les permite reconocer patrones, hacer predicciones y tomar decisiones basadas en ejemplos pasados.

Una de las características destacadas del _Machine Learning_ es su capacidad para hacer predicciones precisas basadas en el análisis de datos, lo que lo convierte en una herramienta poderosa en una amplia variedad de aplicaciones, desde el procesamiento del lenguaje natural hasta el análisis de imágenes y la toma de decisiones automatizada.

Existen tres distintos modelos de predicción de _Machine Learning_: Aprendizaje Supervisado, Aprendizaje No Supervisado y Deep Learning (Aprendizaje Reforzado).

==== Aprendizaje supervisado

El modelo de aprendizaje supervisado implica usar ejemplos etiquetados para que el algoritmo se ajuste y aprenda patrones. Para entrenar, se le provee a la IA un conjunto de datos etiquetados de a pares _input - output_ de forma

stem:[(x_1, y_1), (x_2, y_2),...(x_N, y_N)]

En la que cada _output_ stem:[y_j] fue generado por una función desconocida stem:[y = f(x)].

La IA deberá descubir una función stem:[h(x)] llamada _hipótesis_ que aproxime a stem:[f(x)] lo mejor posible. De esta forma, podrá predecir valores stem:[y] para nuevos stem:[x]. Esta función debe ser  _consistente_, es decir, debe funcionar para todos los datos de entrenamiento. Observemos que esto requiere intervención humana para clasificar y etiquetar datos.

Cuando el _output_ stem:[y_j] es parte de un conjunto finito de valores, el aprendizaje supervisado es llamado _clasificación_. Si el _output_ stem:[y_j] es un número, es llamado _regresión_.

Podemos tener distintas funciones stem:[h(x)] consistentes, por lo que debemos elegir entre una de las candidatas. En este caso, se debe elegir la más simple, ya que en su implementación implica una menor complejidad computacional.

Este tipo de aprendizaje e puede utilizar para filtrado de spam en correos, por ejemplo. En este caso, el algoritmo aprende de correos marcados como spam para identificar patrones y dirigir futuros correos similares a la carpeta de spam.

//checkear que va acá
Otra forma de aprendizaje supervisado son los algoritmos de _árboles de decisión_, siendo ésta la forma más exitosa de machine learning.

Esta técnica consiste en crear un árbol cuyos nodos sean atributos a considerar al tomar una decisión, y cuyas hojas sean la decisión final.
De esta forma, el algoritmo recibe un vector de valores para los atributos, y devuelve una decisión.

Preferentemente, el árbol no debe tener mucha altura, por lo que se debe elegir el más óptimo. Para evitar calcular todos los árboles, el algoritmo utiliza una estrategia de _divide and conquer_: le asigna a cada atributo un nivel de _entropía_, para luego clasificarlos. Los atributos con menor nivel de entropía producen una mayor _ganancia de información_, ya que hacen una mejor división de clases, por lo que son evaluados al inicio. Es decir, el árbol se forma con los nodos ordenados de menor a mayor según su entropía. Luego, en cada decisión se van dividiendo y se repite el proceso para cada subárbol. Si la rama generada es de poca importancia se poda, para no tenerla en cuenta en el proceso de toma de decisiónes.

Para calcular la entropía y obtener la ganancia de información de un atributo, el algoritmo se basa en ejemplos. Para entrenarlo, se debe seleccionar un conjunto de ejemplos no homogéneo, para que el aprendizaje sea lo mas correcto posible. De esta forma se puede evitar la mala clasificación de los atributos.

==== Aprendizaje no supervisado

En el modelo de aprendizaje no supervisado los datos que se incorporan no se etiquetan, ya que se desconoce su estructura. El algoritmo clasifica la información por sí solo. El aprendizaje no supervisado se clasifica en:

*   Clustering: Se agrupan datos sin conocimiento previo de su estructura en grupos con características similares. Los grupos obtenidos destacan patrones inherentes en los datos.

*   Reducción dimensional: Se procesan datos complejos al reducir redundancias y agrupar por características similares, generando información valiosa. Se aplica en estrategias de marketing para definir nichos de mercado, como al seleccionar clientes potenciales basados en comportamientos en redes sociales.

==== Aprendizaje semisupervisado

El modelo de aprendizaje semisupervisado es una combinación entre aprendizaje supervisado y aprendizaje no supervisado. Para llevar a cabo el entrenamiento, se le brinda a la IA un conjunto de datos etiquetados y un conjunto de datos sin etiquetar. Incluso, muchos datos pueden estar etiquetados erróneamente. Este paradigma permite mejorar exactitud del algoritmo, pudiendo usar de ejemplos los datos etiquetados manualmente por una persona y aplicar los conocimientos adquiridos en los datos sin etiquetar. Se utiliza mayormente cuando no disponemos de suficientes datos etiquetados para entrenar a la IA.

El aprendizaje semisupervisado permite trabajar al algoritmos tomando las siguiente suposiciones:

*   Suposición de continuidad
Esta suposición permite generar preferencias en las decisiones tomadas utilizando los elementos etiquetados para, así, consumir datos no etiquetados con una base de cómo interpretarlos. De esta manera permite tener limites de decisiones en redes neuronales de baja densidad.

*   Suposición de grupo
Esta suposición implica que la data consumida genera grupos discretos, y en estos grupos es consistente que varios nodos compartan etiquetas. Lo que le permite entrenar al algoritmo en nuevos casos futuros.

*   Suposiciones múltiples
￼Esta suposicion trabaja bajo el principio de que al consumir data para el entrenamiento se puede delimitar el modelo presentado para poder trabajar con campos de nodos de grandes dimensiones sin tener que consumir datos que no sean necesarios para el modelo presentado. Esto permite al algoritmo a procesar elementos con mucha información sin tener consumir los datasets completos. Ejemplos de esto son algoritmos de reconocimiento de voz o facial, ya que sólo es necesario reconocer patrones de voz específicos de una voz humana, sin la necesidad de procesar todo el espectro de audio.

==== Deep Learning

El Deep Learning o Aprendizaje Reforzado es un subconjunto de lo que es Machine learning que igual que el ya mencionado propone el analisis de patrones para poder aplicarlas en diferentes tareas de manera mas eficiente. Este se distinge del "Machine learning" en el sentido que consume datos de manera cruda, sin tener que haber un paso intermedio para ordenar y recibir y procesar datos sin supervicion humana.

Esta manera de procesar datos permite el entrenamiento del modelo en cantidades grandes a traves de la automatizacion de la lectura de informacion que tiene a mano. A su ves esto agiliza el entrenamiento y permite un entrenamiento mas riguroso. Ejemplos de uso de esta teconología serían identificadores de fotos, rostros u texto. 

Este puede separarse en dos formas de aplicación:

*   Las redes neuronales convolucionales (CNN, por sus siglas en inglés). Esta se utiliza para el reconocimiento y clasificacion de imágenes y videos para identificar elementos que se encuentren en los mismos

*   Las redes neuronales recurrentes (RNN, por sus siglas en inglés). Esta se utiliza para el reconocimiento de vos y del lenguaje natural

=== Proyectos de Data Science ó Big Data

==== Data Science vs Big Data, ¿estamos hablando de lo mismo?

Acá va la info de Data Science y Big Data.

==== ¿Cómo se gestiona un proyecto de Data Science o Big Data?

Acá ponemos sarasa.

== Conclusiones / dificultades

Acá ponemos más sarasa.
